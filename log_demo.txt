running_name: demo
model_path: ./weights/best_model.pth
is_train: True
is_test: True
device: cuda
epochs: 200
batch_size: 8
lr: 0.0003
internal: 100
transform: Compose(
    ToTensor()
    Resize(size=(512, 512), interpolation=bilinear, max_size=None, antialias=True)
)
net: UNet(
  (down_conv1): DoubleConv(
    (double_conv): Sequential(
      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (down_conv2): DoubleConv(
    (double_conv): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (down_conv3): DoubleConv(
    (double_conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (down_conv4): DoubleConv(
    (double_conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (down_conv5): DoubleConv(
    (double_conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (up_transpose1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))
  (up_conv1): DoubleConv(
    (double_conv): Sequential(
      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (up_transpose2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))
  (up_conv2): DoubleConv(
    (double_conv): Sequential(
      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (up_transpose3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
  (up_conv3): DoubleConv(
    (double_conv): Sequential(
      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (up_transpose4): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))
  (up_conv4): DoubleConv(
    (double_conv): Sequential(
      (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
  )
  (out_conv): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
epoch00000, t_loss: 0.064568, v_loss: 0.089974, best_epoch 0: 0.089974
epoch00001, t_loss: 0.026622, v_loss: 0.058045, best_epoch 1: 0.058045
epoch00002, t_loss: 0.018170, v_loss: 0.045639, best_epoch 2: 0.045639
epoch00003, t_loss: 0.016137, v_loss: 0.018278, best_epoch 3: 0.018278
epoch00004, t_loss: 0.013397, v_loss: 0.013837, best_epoch 4: 0.013837
epoch00005, t_loss: 0.013459, v_loss: 0.011117, best_epoch 5: 0.011117
epoch00006, t_loss: 0.014451, v_loss: 0.012486, best_epoch 5: 0.011117
epoch00007, t_loss: 0.012532, v_loss: 0.013475, best_epoch 5: 0.011117
epoch00008, t_loss: 0.014843, v_loss: 0.012093, best_epoch 5: 0.011117
epoch00009, t_loss: 0.011885, v_loss: 0.017134, best_epoch 5: 0.011117
epoch00010, t_loss: 0.013352, v_loss: 0.010837, best_epoch 10: 0.010837
epoch00011, t_loss: 0.012329, v_loss: 0.010730, best_epoch 11: 0.010730
epoch00012, t_loss: 0.013897, v_loss: 0.014298, best_epoch 11: 0.010730
epoch00013, t_loss: 0.011725, v_loss: 0.013450, best_epoch 11: 0.010730
epoch00014, t_loss: 0.012820, v_loss: 0.011341, best_epoch 11: 0.010730
epoch00015, t_loss: 0.011673, v_loss: 0.010163, best_epoch 15: 0.010163
epoch00016, t_loss: 0.009783, v_loss: 0.009631, best_epoch 16: 0.009631
epoch00017, t_loss: 0.011910, v_loss: 0.013820, best_epoch 16: 0.009631
epoch00018, t_loss: 0.012581, v_loss: 0.009602, best_epoch 18: 0.009602
epoch00019, t_loss: 0.011505, v_loss: 0.012985, best_epoch 18: 0.009602
epoch00020, t_loss: 0.011778, v_loss: 0.009540, best_epoch 20: 0.009540
epoch00021, t_loss: 0.010483, v_loss: 0.009585, best_epoch 20: 0.009540
epoch00022, t_loss: 0.010703, v_loss: 0.009894, best_epoch 20: 0.009540
epoch00023, t_loss: 0.013721, v_loss: 0.011574, best_epoch 20: 0.009540
epoch00024, t_loss: 0.018595, v_loss: 0.022165, best_epoch 20: 0.009540
epoch00025, t_loss: 0.011926, v_loss: 0.009986, best_epoch 20: 0.009540
epoch00026, t_loss: 0.010154, v_loss: 0.009901, best_epoch 20: 0.009540
epoch00027, t_loss: 0.011816, v_loss: 0.009594, best_epoch 20: 0.009540
epoch00028, t_loss: 0.011320, v_loss: 0.009533, best_epoch 28: 0.009533
epoch00029, t_loss: 0.009731, v_loss: 0.009641, best_epoch 28: 0.009533
epoch00030, t_loss: 0.010473, v_loss: 0.012940, best_epoch 28: 0.009533
epoch00031, t_loss: 0.010118, v_loss: 0.009777, best_epoch 28: 0.009533
epoch00032, t_loss: 0.011037, v_loss: 0.012703, best_epoch 28: 0.009533
epoch00033, t_loss: 0.010754, v_loss: 0.009433, best_epoch 33: 0.009433
epoch00034, t_loss: 0.010430, v_loss: 0.012044, best_epoch 33: 0.009433
epoch00035, t_loss: 0.011027, v_loss: 0.012904, best_epoch 33: 0.009433
epoch00036, t_loss: 0.010656, v_loss: 0.009401, best_epoch 36: 0.009401
epoch00037, t_loss: 0.011599, v_loss: 0.009390, best_epoch 37: 0.009390
epoch00038, t_loss: 0.011383, v_loss: 0.010116, best_epoch 37: 0.009390
epoch00039, t_loss: 0.010758, v_loss: 0.010107, best_epoch 37: 0.009390
epoch00040, t_loss: 0.010651, v_loss: 0.010514, best_epoch 37: 0.009390
epoch00041, t_loss: 0.009857, v_loss: 0.009358, best_epoch 41: 0.009358
epoch00042, t_loss: 0.011120, v_loss: 0.009697, best_epoch 41: 0.009358
epoch00043, t_loss: 0.009149, v_loss: 0.009356, best_epoch 43: 0.009356
epoch00044, t_loss: 0.010091, v_loss: 0.009103, best_epoch 44: 0.009103
epoch00045, t_loss: 0.011675, v_loss: 0.010452, best_epoch 44: 0.009103
epoch00046, t_loss: 0.010287, v_loss: 0.010209, best_epoch 44: 0.009103
epoch00047, t_loss: 0.009317, v_loss: 0.011633, best_epoch 44: 0.009103
epoch00048, t_loss: 0.009937, v_loss: 0.010940, best_epoch 44: 0.009103
epoch00049, t_loss: 0.009947, v_loss: 0.012378, best_epoch 44: 0.009103
epoch00050, t_loss: 0.010291, v_loss: 0.009242, best_epoch 44: 0.009103
epoch00051, t_loss: 0.009584, v_loss: 0.009656, best_epoch 44: 0.009103
epoch00052, t_loss: 0.010010, v_loss: 0.015713, best_epoch 44: 0.009103
epoch00053, t_loss: 0.009683, v_loss: 0.013055, best_epoch 44: 0.009103
epoch00054, t_loss: 0.009311, v_loss: 0.009004, best_epoch 54: 0.009004
epoch00055, t_loss: 0.008957, v_loss: 0.009802, best_epoch 54: 0.009004
epoch00056, t_loss: 0.009073, v_loss: 0.009781, best_epoch 54: 0.009004
epoch00057, t_loss: 0.008728, v_loss: 0.009360, best_epoch 54: 0.009004
epoch00058, t_loss: 0.008039, v_loss: 0.009938, best_epoch 54: 0.009004
epoch00059, t_loss: 0.009302, v_loss: 0.009701, best_epoch 54: 0.009004
epoch00060, t_loss: 0.008101, v_loss: 0.009604, best_epoch 54: 0.009004
epoch00061, t_loss: 0.007909, v_loss: 0.009761, best_epoch 54: 0.009004
epoch00062, t_loss: 0.007731, v_loss: 0.009571, best_epoch 54: 0.009004
epoch00063, t_loss: 0.008828, v_loss: 0.010241, best_epoch 54: 0.009004
epoch00064, t_loss: 0.008359, v_loss: 0.010372, best_epoch 54: 0.009004
epoch00065, t_loss: 0.009241, v_loss: 0.010165, best_epoch 54: 0.009004
epoch00066, t_loss: 0.008467, v_loss: 0.012856, best_epoch 54: 0.009004
epoch00067, t_loss: 0.008482, v_loss: 0.017029, best_epoch 54: 0.009004
epoch00068, t_loss: 0.009212, v_loss: 0.009624, best_epoch 54: 0.009004
epoch00069, t_loss: 0.009829, v_loss: 0.011116, best_epoch 54: 0.009004
epoch00070, t_loss: 0.008924, v_loss: 0.009563, best_epoch 54: 0.009004
epoch00071, t_loss: 0.008438, v_loss: 0.012624, best_epoch 54: 0.009004
epoch00072, t_loss: 0.008553, v_loss: 0.010209, best_epoch 54: 0.009004
epoch00073, t_loss: 0.008913, v_loss: 0.009778, best_epoch 54: 0.009004
epoch00074, t_loss: 0.007246, v_loss: 0.010169, best_epoch 54: 0.009004
epoch00075, t_loss: 0.007918, v_loss: 0.011104, best_epoch 54: 0.009004
epoch00076, t_loss: 0.007894, v_loss: 0.009595, best_epoch 54: 0.009004
epoch00077, t_loss: 0.007287, v_loss: 0.012470, best_epoch 54: 0.009004
epoch00078, t_loss: 0.008878, v_loss: 0.011249, best_epoch 54: 0.009004
epoch00079, t_loss: 0.007624, v_loss: 0.013568, best_epoch 54: 0.009004
epoch00080, t_loss: 0.007596, v_loss: 0.009706, best_epoch 54: 0.009004
epoch00081, t_loss: 0.007986, v_loss: 0.009051, best_epoch 54: 0.009004
epoch00082, t_loss: 0.007752, v_loss: 0.010805, best_epoch 54: 0.009004
epoch00083, t_loss: 0.006911, v_loss: 0.009336, best_epoch 54: 0.009004
epoch00084, t_loss: 0.007200, v_loss: 0.009843, best_epoch 54: 0.009004
epoch00085, t_loss: 0.006579, v_loss: 0.009533, best_epoch 54: 0.009004
epoch00086, t_loss: 0.006963, v_loss: 0.009138, best_epoch 54: 0.009004
epoch00087, t_loss: 0.006576, v_loss: 0.011372, best_epoch 54: 0.009004
epoch00088, t_loss: 0.006158, v_loss: 0.009351, best_epoch 54: 0.009004
epoch00089, t_loss: 0.006391, v_loss: 0.009504, best_epoch 54: 0.009004
epoch00090, t_loss: 0.007225, v_loss: 0.010482, best_epoch 54: 0.009004
epoch00091, t_loss: 0.006654, v_loss: 0.009634, best_epoch 54: 0.009004
epoch00092, t_loss: 0.006235, v_loss: 0.009496, best_epoch 54: 0.009004
epoch00093, t_loss: 0.005630, v_loss: 0.012093, best_epoch 54: 0.009004
epoch00094, t_loss: 0.006248, v_loss: 0.009735, best_epoch 54: 0.009004
epoch00095, t_loss: 0.006137, v_loss: 0.010164, best_epoch 54: 0.009004
epoch00096, t_loss: 0.005530, v_loss: 0.010037, best_epoch 54: 0.009004
epoch00097, t_loss: 0.005951, v_loss: 0.011206, best_epoch 54: 0.009004
epoch00098, t_loss: 0.005881, v_loss: 0.011138, best_epoch 54: 0.009004
epoch00099, t_loss: 0.005786, v_loss: 0.011627, best_epoch 54: 0.009004
epoch00100, t_loss: 0.007074, v_loss: 0.011322, best_epoch 54: 0.009004
epoch00101, t_loss: 0.008330, v_loss: 0.011839, best_epoch 54: 0.009004
epoch00102, t_loss: 0.007186, v_loss: 0.016079, best_epoch 54: 0.009004
epoch00103, t_loss: 0.007210, v_loss: 0.011368, best_epoch 54: 0.009004
epoch00104, t_loss: 0.006279, v_loss: 0.011281, best_epoch 54: 0.009004
epoch00105, t_loss: 0.005645, v_loss: 0.010327, best_epoch 54: 0.009004
epoch00106, t_loss: 0.005503, v_loss: 0.009478, best_epoch 54: 0.009004
epoch00107, t_loss: 0.005627, v_loss: 0.010623, best_epoch 54: 0.009004
epoch00108, t_loss: 0.005289, v_loss: 0.009650, best_epoch 54: 0.009004
epoch00109, t_loss: 0.006034, v_loss: 0.009386, best_epoch 54: 0.009004
epoch00110, t_loss: 0.005682, v_loss: 0.010752, best_epoch 54: 0.009004
epoch00111, t_loss: 0.006341, v_loss: 0.009798, best_epoch 54: 0.009004
epoch00112, t_loss: 0.006027, v_loss: 0.009512, best_epoch 54: 0.009004
epoch00113, t_loss: 0.005443, v_loss: 0.009776, best_epoch 54: 0.009004
epoch00114, t_loss: 0.005564, v_loss: 0.010487, best_epoch 54: 0.009004
epoch00115, t_loss: 0.006521, v_loss: 0.011101, best_epoch 54: 0.009004
epoch00116, t_loss: 0.005430, v_loss: 0.009864, best_epoch 54: 0.009004
epoch00117, t_loss: 0.005349, v_loss: 0.010151, best_epoch 54: 0.009004
epoch00118, t_loss: 0.005193, v_loss: 0.010275, best_epoch 54: 0.009004
epoch00119, t_loss: 0.005219, v_loss: 0.009422, best_epoch 54: 0.009004
epoch00120, t_loss: 0.005193, v_loss: 0.009904, best_epoch 54: 0.009004
epoch00121, t_loss: 0.004949, v_loss: 0.010029, best_epoch 54: 0.009004
epoch00122, t_loss: 0.004995, v_loss: 0.009426, best_epoch 54: 0.009004
epoch00123, t_loss: 0.005048, v_loss: 0.010142, best_epoch 54: 0.009004
epoch00124, t_loss: 0.004768, v_loss: 0.009531, best_epoch 54: 0.009004
epoch00125, t_loss: 0.004886, v_loss: 0.009633, best_epoch 54: 0.009004
epoch00126, t_loss: 0.004574, v_loss: 0.009826, best_epoch 54: 0.009004
epoch00127, t_loss: 0.004587, v_loss: 0.009548, best_epoch 54: 0.009004
epoch00128, t_loss: 0.004543, v_loss: 0.009844, best_epoch 54: 0.009004
epoch00129, t_loss: 0.004879, v_loss: 0.010556, best_epoch 54: 0.009004
epoch00130, t_loss: 0.004817, v_loss: 0.009832, best_epoch 54: 0.009004
epoch00131, t_loss: 0.004554, v_loss: 0.010234, best_epoch 54: 0.009004
epoch00132, t_loss: 0.004183, v_loss: 0.010437, best_epoch 54: 0.009004
epoch00133, t_loss: 0.004278, v_loss: 0.010153, best_epoch 54: 0.009004
epoch00134, t_loss: 0.004335, v_loss: 0.009747, best_epoch 54: 0.009004
epoch00135, t_loss: 0.004080, v_loss: 0.010016, best_epoch 54: 0.009004
epoch00136, t_loss: 0.004056, v_loss: 0.009437, best_epoch 54: 0.009004
epoch00137, t_loss: 0.003940, v_loss: 0.010530, best_epoch 54: 0.009004
epoch00138, t_loss: 0.004272, v_loss: 0.011215, best_epoch 54: 0.009004
epoch00139, t_loss: 0.004205, v_loss: 0.010101, best_epoch 54: 0.009004
epoch00140, t_loss: 0.004139, v_loss: 0.010269, best_epoch 54: 0.009004
epoch00141, t_loss: 0.004289, v_loss: 0.011241, best_epoch 54: 0.009004
epoch00142, t_loss: 0.003991, v_loss: 0.010361, best_epoch 54: 0.009004
epoch00143, t_loss: 0.003920, v_loss: 0.010215, best_epoch 54: 0.009004
epoch00144, t_loss: 0.004144, v_loss: 0.010107, best_epoch 54: 0.009004
epoch00145, t_loss: 0.003962, v_loss: 0.009805, best_epoch 54: 0.009004
epoch00146, t_loss: 0.003742, v_loss: 0.010234, best_epoch 54: 0.009004
epoch00147, t_loss: 0.003788, v_loss: 0.009593, best_epoch 54: 0.009004
epoch00148, t_loss: 0.003951, v_loss: 0.009849, best_epoch 54: 0.009004
epoch00149, t_loss: 0.003864, v_loss: 0.010084, best_epoch 54: 0.009004
epoch00150, t_loss: 0.003943, v_loss: 0.009970, best_epoch 54: 0.009004
epoch00151, t_loss: 0.003541, v_loss: 0.011354, best_epoch 54: 0.009004
epoch00152, t_loss: 0.004055, v_loss: 0.010612, best_epoch 54: 0.009004
epoch00153, t_loss: 0.003922, v_loss: 0.010586, best_epoch 54: 0.009004
epoch00154, t_loss: 0.003982, v_loss: 0.011126, best_epoch 54: 0.009004
epoch00155, t_loss: 0.003941, v_loss: 0.011478, best_epoch 54: 0.009004
epoch00156, t_loss: 0.003763, v_loss: 0.010496, best_epoch 54: 0.009004
epoch00157, t_loss: 0.003859, v_loss: 0.009862, best_epoch 54: 0.009004
epoch00158, t_loss: 0.003595, v_loss: 0.009788, best_epoch 54: 0.009004
epoch00159, t_loss: 0.003537, v_loss: 0.010210, best_epoch 54: 0.009004
epoch00160, t_loss: 0.003967, v_loss: 0.010310, best_epoch 54: 0.009004
epoch00161, t_loss: 0.003919, v_loss: 0.010171, best_epoch 54: 0.009004
epoch00162, t_loss: 0.003859, v_loss: 0.011087, best_epoch 54: 0.009004
epoch00163, t_loss: 0.004120, v_loss: 0.012498, best_epoch 54: 0.009004
epoch00164, t_loss: 0.003594, v_loss: 0.010487, best_epoch 54: 0.009004
epoch00165, t_loss: 0.003781, v_loss: 0.010027, best_epoch 54: 0.009004
epoch00166, t_loss: 0.003358, v_loss: 0.009608, best_epoch 54: 0.009004
epoch00167, t_loss: 0.003649, v_loss: 0.013406, best_epoch 54: 0.009004
epoch00168, t_loss: 0.003755, v_loss: 0.015504, best_epoch 54: 0.009004
epoch00169, t_loss: 0.003497, v_loss: 0.010769, best_epoch 54: 0.009004
epoch00170, t_loss: 0.003575, v_loss: 0.010183, best_epoch 54: 0.009004
epoch00171, t_loss: 0.003371, v_loss: 0.010273, best_epoch 54: 0.009004
epoch00172, t_loss: 0.003624, v_loss: 0.010368, best_epoch 54: 0.009004
epoch00173, t_loss: 0.003300, v_loss: 0.010416, best_epoch 54: 0.009004
epoch00174, t_loss: 0.003555, v_loss: 0.011360, best_epoch 54: 0.009004
epoch00175, t_loss: 0.003531, v_loss: 0.010429, best_epoch 54: 0.009004
epoch00176, t_loss: 0.003763, v_loss: 0.011538, best_epoch 54: 0.009004
epoch00177, t_loss: 0.003720, v_loss: 0.010871, best_epoch 54: 0.009004
epoch00178, t_loss: 0.003276, v_loss: 0.010086, best_epoch 54: 0.009004
epoch00179, t_loss: 0.003328, v_loss: 0.010071, best_epoch 54: 0.009004
epoch00180, t_loss: 0.003047, v_loss: 0.010104, best_epoch 54: 0.009004
epoch00181, t_loss: 0.003236, v_loss: 0.009897, best_epoch 54: 0.009004
epoch00182, t_loss: 0.003445, v_loss: 0.010433, best_epoch 54: 0.009004
epoch00183, t_loss: 0.003339, v_loss: 0.010929, best_epoch 54: 0.009004
epoch00184, t_loss: 0.003509, v_loss: 0.010786, best_epoch 54: 0.009004
epoch00185, t_loss: 0.003340, v_loss: 0.010308, best_epoch 54: 0.009004
epoch00186, t_loss: 0.003124, v_loss: 0.010005, best_epoch 54: 0.009004
epoch00187, t_loss: 0.003442, v_loss: 0.010039, best_epoch 54: 0.009004
epoch00188, t_loss: 0.003284, v_loss: 0.010701, best_epoch 54: 0.009004
epoch00189, t_loss: 0.003679, v_loss: 0.012594, best_epoch 54: 0.009004
epoch00190, t_loss: 0.004002, v_loss: 0.012822, best_epoch 54: 0.009004
epoch00191, t_loss: 0.004079, v_loss: 0.011666, best_epoch 54: 0.009004
epoch00192, t_loss: 0.004099, v_loss: 0.010109, best_epoch 54: 0.009004
epoch00193, t_loss: 0.004412, v_loss: 0.011638, best_epoch 54: 0.009004
epoch00194, t_loss: 0.004024, v_loss: 0.011691, best_epoch 54: 0.009004
epoch00195, t_loss: 0.003678, v_loss: 0.010922, best_epoch 54: 0.009004
epoch00196, t_loss: 0.003157, v_loss: 0.010056, best_epoch 54: 0.009004
epoch00197, t_loss: 0.003058, v_loss: 0.010140, best_epoch 54: 0.009004
epoch00198, t_loss: 0.003299, v_loss: 0.010407, best_epoch 54: 0.009004
epoch00199, t_loss: 0.003258, v_loss: 0.010116, best_epoch 54: 0.009004
t_loss_set:
[0.06456825276836753, 0.02662200729052226, 0.01817019438991944, 0.01613748741025726, 0.013396826727936665, 0.013459097128361464, 0.014450534091641506, 0.01253154780715704, 0.014843204834808906, 0.011884610013415417, 0.013352291736130914, 0.012328932294622064, 0.013896940896908442, 0.011725470113257567, 0.012820036072904864, 0.011673368824024996, 0.009783057340731224, 0.011910011060535908, 0.01258073141798377, 0.011504725087434053, 0.011777745482201377, 0.01048331925024589, 0.010702713315064708, 0.013720807386562228, 0.01859523736250897, 0.01192579255439341, 0.010153585113584995, 0.011815703085934123, 0.011319951076681415, 0.00973090009453396, 0.010473323636688292, 0.010118280188180506, 0.011037199253526827, 0.010754343199854096, 0.010429708442340294, 0.011027077406955263, 0.010656344005838037, 0.011599145364016294, 0.011382628581486642, 0.010758406094585856, 0.010650556883774698, 0.009857022281115254, 0.011120376526378095, 0.00914937308213363, 0.010090521420352161, 0.011674946794907251, 0.010286922721813122, 0.009316562092863023, 0.00993697096904119, 0.009947338645967344, 0.010291445263040563, 0.009583739059356352, 0.010010450379922986, 0.009683407532672087, 0.009310892783105373, 0.008957430720329285, 0.009072647235977152, 0.008728111744858325, 0.008039028655427197, 0.009302144404500723, 0.008101095911115408, 0.007908854400739074, 0.007731263758614659, 0.008828353136777878, 0.008359444482872883, 0.009241134976036847, 0.008466802615051469, 0.00848221177390466, 0.009212241081210474, 0.00982898815224568, 0.008924491121433675, 0.008438323663237194, 0.008553161285817623, 0.00891291726535807, 0.00724554651727279, 0.007917716982774436, 0.007893917616456747, 0.007287205041696628, 0.008877711409392456, 0.007624264069211979, 0.007595628189543883, 0.007985750096850097, 0.007751818435887496, 0.006910873809829354, 0.007200168639731904, 0.006578807292195658, 0.00696333684027195, 0.0065756971792628365, 0.006158307582760851, 0.006391085451468825, 0.0072245633928105235, 0.006653972358132402, 0.006235005528045197, 0.005629862658679485, 0.006248055685621996, 0.006136713937545816, 0.005530172435101122, 0.005951418269736071, 0.005880650288114945, 0.005786474444903433, 0.00707392367379119, 0.00833038609319677, 0.007185771673296888, 0.007210182608105242, 0.0062787283677607775, 0.005644972901791334, 0.005503170677305509, 0.005627383245155215, 0.005288834198533247, 0.006033906887751073, 0.005681913462467492, 0.00634070086137702, 0.0060268493834882975, 0.005443320473811279, 0.0055644681172755854, 0.0065205909389381604, 0.005429861058170597, 0.0053489173296839, 0.005192665771270792, 0.005218546274894227, 0.005192731487719963, 0.004949036558779578, 0.004994917777366936, 0.005048028853101035, 0.0047676760974961025, 0.004886300361249596, 0.004574174798714618, 0.004587114051294823, 0.004542662063613534, 0.0048788900797565775, 0.00481731875333935, 0.0045538490715747075, 0.004182545992080122, 0.004277912220762421, 0.0043347822890306515, 0.004080227809026837, 0.004055637575220317, 0.003939781376781563, 0.004272005637176335, 0.004204989701975137, 0.0041393858652251465, 0.0042889773030765355, 0.003990972356405109, 0.003919507124616454, 0.00414385045102487, 0.003962417792839308, 0.0037419357298252485, 0.003787664172705263, 0.003950842248741537, 0.003863581883100172, 0.003943070264843603, 0.0035406786870832243, 0.004055278841406107, 0.003922401985619217, 0.003981877913853775, 0.003940625368462254, 0.0037625936480859914, 0.0038594240904785693, 0.0035951470296519497, 0.003536845479781429, 0.003966809172804157, 0.003919131762813777, 0.003858830973816415, 0.004119791672565043, 0.0035944205786411962, 0.003780935522324095, 0.003357732125247518, 0.003648796060588211, 0.003755243281678607, 0.0034965414088219404, 0.003574854985345155, 0.0033708535484038293, 0.003624289898046603, 0.003299616121997436, 0.003554587524073819, 0.0035306569964935384, 0.003762827351844559, 0.003719590255059302, 0.0032759813281397023, 0.0033283669326920062, 0.0030471992407304547, 0.0032360553353404007, 0.0034447299549356103, 0.0033390674895296493, 0.0035087435001817844, 0.003339840235033383, 0.0031244924757629633, 0.0034422381043744585, 0.0032837777010475597, 0.0036791107850149274, 0.004002427585267772, 0.004079394779788951, 0.004098514288974305, 0.004412257811054587, 0.00402400045034786, 0.003678204996200899, 0.003157082033188393, 0.0030578188792181513, 0.003298585186712444, 0.0032582132068152228]
v_loss_set:
[0.08997420221567154, 0.058045318350195885, 0.04563935846090317, 0.018278432078659534, 0.013837236445397139, 0.01111657451838255, 0.012486326973885298, 0.013474959880113602, 0.012092874851077795, 0.0171338627114892, 0.010837467852979898, 0.010729867033660412, 0.014298407826572657, 0.013449867255985737, 0.011341143865138292, 0.010162828490138054, 0.009631335968151689, 0.01382047962397337, 0.009602011879906058, 0.012984707485884428, 0.009539749938994646, 0.009585390333086252, 0.009893831331282854, 0.011574259027838707, 0.02216487191617489, 0.009986072778701782, 0.009901075391098857, 0.009593597380444407, 0.009533469565212727, 0.009640796575695276, 0.012940358370542526, 0.009777013678103685, 0.012702811043709517, 0.009432593593373895, 0.012044320814311504, 0.012903913855552673, 0.009400524199008942, 0.009390317369252443, 0.010116290301084518, 0.010107498615980148, 0.010514215100556612, 0.00935796508565545, 0.009697346482425928, 0.009355625603348017, 0.009103049524128437, 0.010451850946992636, 0.01020932849496603, 0.011632516048848629, 0.010939905419945717, 0.012377787847071886, 0.009242493659257889, 0.009656483540311456, 0.015712596476078033, 0.013054761569947004, 0.009004157036542892, 0.009801599895581603, 0.009780713124200702, 0.00935998186469078, 0.009938461240381002, 0.009700922295451164, 0.009604383958503604, 0.009760601446032524, 0.009571332018822432, 0.0102410102263093, 0.010372376535087824, 0.010165269952267408, 0.012856450863182545, 0.017028752714395523, 0.009623879799619317, 0.011116039473563433, 0.009563278639689088, 0.012623861897736788, 0.010209214873611927, 0.009778339881449938, 0.01016856450587511, 0.011103610508143902, 0.009594751056283712, 0.012469504959881306, 0.011248638853430748, 0.013567700516432524, 0.009705677861347795, 0.009051017463207245, 0.010804615914821625, 0.009336370509117842, 0.009842928033322096, 0.0095330816693604, 0.009138021618127823, 0.011372069362550974, 0.009351079585030675, 0.009503809502348304, 0.010481784120202065, 0.009633579291403294, 0.009496421786025167, 0.012092594988644123, 0.009734660619869828, 0.010164433624595404, 0.010037173051387072, 0.011205550748854876, 0.011137936264276505, 0.01162685314193368, 0.011322098318487406, 0.011839238461107016, 0.01607881672680378, 0.011367636267095804, 0.011281385086476803, 0.010327403200790286, 0.009478219784796238, 0.010623308829963207, 0.009650173829868436, 0.00938616693019867, 0.010751726571470499, 0.009797862265259027, 0.009511854499578476, 0.009776395745575428, 0.010487192310392857, 0.011100898496806622, 0.00986386532895267, 0.010150619316846132, 0.010275273118168116, 0.009421717841178179, 0.00990443886257708, 0.010029256343841553, 0.009425977477803826, 0.010141847655177116, 0.009530958253890276, 0.009633161593228579, 0.009826424065977335, 0.009547593537718058, 0.009843896143138409, 0.010555850807577372, 0.009832263924181461, 0.010233663022518158, 0.010437170043587685, 0.010152820032089949, 0.009746807161718607, 0.010015754727646708, 0.009437112137675285, 0.01053043082356453, 0.011214514262974262, 0.010101305320858955, 0.010269452817738056, 0.011240802705287933, 0.010360805783420801, 0.010215398855507374, 0.010106627363711596, 0.009805349865928292, 0.010233621578663588, 0.009593055583536625, 0.00984907615929842, 0.010083962697535753, 0.009970334824174643, 0.01135366689413786, 0.010612276382744312, 0.010585646145045757, 0.011126308236271143, 0.011477745603770018, 0.01049557700753212, 0.009861503262072802, 0.009788262657821178, 0.010209780419245362, 0.01030989270657301, 0.010171234840527177, 0.011087232269346714, 0.012498350348323584, 0.010486710350960493, 0.01002666400745511, 0.009608120191842318, 0.013406263664364815, 0.015503836795687675, 0.010769437067210674, 0.010182548547163606, 0.010272846557199955, 0.010367855429649353, 0.010416275355964899, 0.011359994765371084, 0.010428732261061668, 0.011538460850715637, 0.0108714341185987, 0.010086300782859325, 0.010071052936837077, 0.010103916516527534, 0.009896630654111505, 0.010433027055114508, 0.010928881354629993, 0.010786321945488453, 0.010307720163837075, 0.010004879208281636, 0.01003852323628962, 0.010701228864490986, 0.012594106141477823, 0.012822296470403671, 0.011666011065244675, 0.010108537506312132, 0.011637801304459572, 0.011691202409565449, 0.010922267101705074, 0.010055797640234232, 0.010139886755496264, 0.01040745759382844, 0.010115591576322913]
